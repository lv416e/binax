{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BinAX Demo: JAX-Based Reinforcement Learning for Bin Packing\n",
    "\n",
    "This notebook demonstrates the capabilities of BinAX, a high-performance reinforcement learning framework for solving bin packing problems using JAX.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Environment**: 1D bin packing with configurable parameters\n",
    "- **Algorithm**: Proximal Policy Optimization (PPO)\n",
    "- **Networks**: Attention-based and simple architectures\n",
    "- **Framework**: JAX with JIT compilation and vectorization\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install jax jaxlib flax optax chex matplotlib seaborn tqdm\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from jax import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"JAX backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BinAX components\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from binax import BinPackingEnv, PPOAgent\n",
    "from binax.algorithms import PPOConfig\n",
    "from binax.networks import create_network\n",
    "from binax.trainer import TrainingConfig, Trainer\n",
    "from binax.types import BinPackingAction, BinPackingState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Exploration\n",
    "\n",
    "Let's start by exploring the bin packing environment and understanding its components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple bin packing environment\n",
    "env = BinPackingEnv(\n",
    "    bin_capacity=1.0, max_bins=10, max_items=20, item_size_range=(0.1, 0.5)\n",
    ")\n",
    "\n",
    "# Initialize environment\n",
    "key = random.PRNGKey(42)\n",
    "state = env.reset(key, num_items=8)\n",
    "\n",
    "print(\"Initial Environment State:\")\n",
    "print(env.render_state(state))\n",
    "print(f\"\\nItem queue: {state.item_queue[:8]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the initial state\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot bin capacities\n",
    "ax1.bar(range(env.max_bins), state.bin_capacities, alpha=0.7, color=\"lightblue\")\n",
    "ax1.set_xlabel(\"Bin Index\")\n",
    "ax1.set_ylabel(\"Remaining Capacity\")\n",
    "ax1.set_title(\"Bin Capacities\")\n",
    "ax1.set_ylim(0, env.bin_capacity * 1.1)\n",
    "\n",
    "# Plot items to pack\n",
    "items_to_pack = state.item_queue[state.item_queue > 0]\n",
    "ax2.bar(range(len(items_to_pack)), items_to_pack, alpha=0.7, color=\"lightcoral\")\n",
    "ax2.set_xlabel(\"Item Index\")\n",
    "ax2.set_ylabel(\"Item Size\")\n",
    "ax2.set_title(\"Items to Pack\")\n",
    "ax2.set_ylim(0, max(items_to_pack) * 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Manual Environment Interaction\n",
    "\n",
    "Let's manually interact with the environment to understand the dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's manually pack items using a simple First Fit strategy\n",
    "def first_fit_strategy(state: BinPackingState) -> int:\n",
    "    \"\"\"Simple First Fit strategy for comparison.\"\"\"\n",
    "    current_item_size = state.item_queue[state.current_item_idx]\n",
    "\n",
    "    # Find first bin that can fit the item\n",
    "    for i, capacity in enumerate(state.bin_capacities):\n",
    "        if capacity >= current_item_size:\n",
    "            return i\n",
    "\n",
    "    # If no bin can fit, return the first empty bin\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Run First Fit on our environment\n",
    "current_state = state\n",
    "episode_rewards = []\n",
    "episode_states = [current_state]\n",
    "\n",
    "print(\"Running First Fit Strategy:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "while not current_state.done:\n",
    "    # Get valid actions\n",
    "    valid_actions = env.get_valid_actions(current_state)\n",
    "\n",
    "    # Use First Fit strategy\n",
    "    action_idx = first_fit_strategy(current_state)\n",
    "    action = BinPackingAction(bin_idx=action_idx)\n",
    "\n",
    "    # Step environment\n",
    "    key, step_key = random.split(key)\n",
    "    next_state, reward, done = env.step(current_state, action, step_key)\n",
    "\n",
    "    print(\n",
    "        f\"Step {current_state.step_count}: Item {current_state.item_queue[current_state.current_item_idx]:.3f} -> Bin {action_idx}, Reward: {reward:.2f}\"\n",
    "    )\n",
    "\n",
    "    episode_rewards.append(reward)\n",
    "    episode_states.append(next_state)\n",
    "    current_state = next_state\n",
    "\n",
    "    if current_state.step_count > 20:  # Safety break\n",
    "        break\n",
    "\n",
    "print(\"\\nFinal State:\")\n",
    "print(env.render_state(current_state))\n",
    "print(f\"\\nTotal Reward: {sum(episode_rewards):.2f}\")\n",
    "print(f\"Bins Used: {jnp.sum(current_state.bin_utilization > 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the packing result\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot final bin utilization\n",
    "used_bins = current_state.bin_utilization > 0\n",
    "bin_indices = np.arange(len(current_state.bin_utilization))\n",
    "used_capacity = env.bin_capacity - current_state.bin_capacities\n",
    "\n",
    "colors = [\"lightgreen\" if used else \"lightgray\" for used in used_bins]\n",
    "ax1.bar(bin_indices, used_capacity, alpha=0.7, color=colors)\n",
    "ax1.axhline(\n",
    "    y=env.bin_capacity, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Capacity\"\n",
    ")\n",
    "ax1.set_xlabel(\"Bin Index\")\n",
    "ax1.set_ylabel(\"Used Capacity\")\n",
    "ax1.set_title(\"Final Bin Utilization\")\n",
    "ax1.legend()\n",
    "\n",
    "# Plot rewards over time\n",
    "ax2.plot(episode_rewards, marker=\"o\", alpha=0.7)\n",
    "ax2.set_xlabel(\"Step\")\n",
    "ax2.set_ylabel(\"Reward\")\n",
    "ax2.set_title(\"Rewards Over Time\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate efficiency metrics\n",
    "total_item_volume = jnp.sum(state.item_queue[state.item_queue > 0])\n",
    "bins_used = jnp.sum(current_state.bin_utilization > 0)\n",
    "total_bin_volume = bins_used * env.bin_capacity\n",
    "efficiency = total_item_volume / total_bin_volume if total_bin_volume > 0 else 0\n",
    "\n",
    "print(\"\\nEfficiency Metrics:\")\n",
    "print(f\"Total item volume: {total_item_volume:.3f}\")\n",
    "print(f\"Bins used: {bins_used}\")\n",
    "print(f\"Total bin volume: {total_bin_volume:.3f}\")\n",
    "print(f\"Packing efficiency: {efficiency:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Network Architecture\n",
    "\n",
    "Let's explore the neural network architectures available in BinAX.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different network architectures\n",
    "attention_network = create_network(\n",
    "    network_type=\"attention\",\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    num_heads=4,\n",
    "    max_bins=10,\n",
    "    dropout_rate=0.1,\n",
    ")\n",
    "\n",
    "simple_network = create_network(\n",
    "    network_type=\"simple\", hidden_dim=128, max_bins=10, dropout_rate=0.1\n",
    ")\n",
    "\n",
    "print(\"Network Architectures Created:\")\n",
    "print(f\"- Attention Network: {attention_network}\")\n",
    "print(f\"- Simple Network: {simple_network}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network parameters and test forward pass\n",
    "key, init_key = random.split(key)\n",
    "dummy_state = env.reset(init_key, num_items=5)\n",
    "\n",
    "# Initialize both networks\n",
    "key, param_key1, param_key2 = random.split(key, 3)\n",
    "attention_params = attention_network.init(param_key1, dummy_state, training=False)\n",
    "simple_params = simple_network.init(param_key2, dummy_state, training=False)\n",
    "\n",
    "# Test forward pass\n",
    "attention_output = attention_network.apply(\n",
    "    attention_params, dummy_state, training=False\n",
    ")\n",
    "simple_output = simple_network.apply(simple_params, dummy_state, training=False)\n",
    "\n",
    "print(\"Network Outputs:\")\n",
    "print(\n",
    "    f\"Attention Network - Action Logits Shape: {attention_output.action_logits.shape}\"\n",
    ")\n",
    "print(f\"Attention Network - Value: {attention_output.value:.4f}\")\n",
    "print(f\"Simple Network - Action Logits Shape: {simple_output.action_logits.shape}\")\n",
    "print(f\"Simple Network - Value: {simple_output.value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize network predictions\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot action logits\n",
    "ax1.bar(\n",
    "    range(len(attention_output.action_logits)),\n",
    "    attention_output.action_logits,\n",
    "    alpha=0.7,\n",
    "    color=\"blue\",\n",
    ")\n",
    "ax1.set_xlabel(\"Action (Bin Index)\")\n",
    "ax1.set_ylabel(\"Logit Value\")\n",
    "ax1.set_title(\"Attention Network - Action Logits\")\n",
    "\n",
    "ax2.bar(\n",
    "    range(len(simple_output.action_logits)),\n",
    "    simple_output.action_logits,\n",
    "    alpha=0.7,\n",
    "    color=\"red\",\n",
    ")\n",
    "ax2.set_xlabel(\"Action (Bin Index)\")\n",
    "ax2.set_ylabel(\"Logit Value\")\n",
    "ax2.set_title(\"Simple Network - Action Logits\")\n",
    "\n",
    "# Plot action probabilities\n",
    "attention_probs = jax.nn.softmax(attention_output.action_logits)\n",
    "simple_probs = jax.nn.softmax(simple_output.action_logits)\n",
    "\n",
    "ax3.bar(range(len(attention_probs)), attention_probs, alpha=0.7, color=\"blue\")\n",
    "ax3.set_xlabel(\"Action (Bin Index)\")\n",
    "ax3.set_ylabel(\"Probability\")\n",
    "ax3.set_title(\"Attention Network - Action Probabilities\")\n",
    "\n",
    "ax4.bar(range(len(simple_probs)), simple_probs, alpha=0.7, color=\"red\")\n",
    "ax4.set_xlabel(\"Action (Bin Index)\")\n",
    "ax4.set_ylabel(\"Probability\")\n",
    "ax4.set_title(\"Simple Network - Action Probabilities\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"\\nCurrent item size: {dummy_state.item_queue[dummy_state.current_item_idx]:.3f}\"\n",
    ")\n",
    "print(f\"Bin capacities: {dummy_state.bin_capacities[:5]}\")\n",
    "print(f\"Valid actions: {env.get_valid_actions(dummy_state)[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PPO Agent Demo\n",
    "\n",
    "Let's create a PPO agent and demonstrate its action selection process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PPO agent\n",
    "ppo_config = PPOConfig(\n",
    "    learning_rate=3e-4,\n",
    "    num_epochs=4,\n",
    "    num_minibatches=4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_eps=0.2,\n",
    "    entropy_coeff=0.01,\n",
    "    value_loss_coeff=0.5,\n",
    ")\n",
    "\n",
    "agent = PPOAgent(attention_network, ppo_config, action_dim=11)  # max_bins + 1\n",
    "\n",
    "# Initialize agent parameters\n",
    "key, agent_key = random.split(key)\n",
    "agent_params = agent.init_params(agent_key, dummy_state)\n",
    "\n",
    "print(\"PPO Agent initialized successfully!\")\n",
    "print(f\"Number of parameters: {sum(x.size for x in jax.tree_leaves(agent_params)):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate agent action selection\n",
    "test_state = env.reset(random.PRNGKey(123), num_items=6)\n",
    "valid_actions = env.get_valid_actions(test_state)\n",
    "\n",
    "print(\"Agent Action Selection Demo:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for i in range(5):\n",
    "    key, action_key = random.split(key)\n",
    "    action, log_prob, value = agent.select_action(\n",
    "        agent_params, test_state, action_key, valid_actions\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Sample {i + 1}: Action={action.bin_idx}, Log Prob={log_prob:.4f}, Value={value:.4f}\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nCurrent item size: {test_state.item_queue[test_state.current_item_idx]:.3f}\")\n",
    "print(f\"Valid actions: {valid_actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Mini Training Demo\n",
    "\n",
    "Let's run a small-scale training demo to see the agent learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple training configuration for demo\n",
    "demo_config = TrainingConfig(\n",
    "    # Small scale for demo\n",
    "    total_timesteps=10_000,\n",
    "    num_envs=8,\n",
    "    rollout_length=64,\n",
    "    # Environment\n",
    "    bin_capacity=1.0,\n",
    "    max_bins=10,\n",
    "    max_items=15,\n",
    "    item_size_range=(0.1, 0.4),\n",
    "    # Network\n",
    "    network_type=\"simple\",  # Faster for demo\n",
    "    hidden_dim=64,\n",
    "    num_layers=2,\n",
    "    # Training\n",
    "    learning_rate=1e-3,\n",
    "    num_epochs=2,\n",
    "    num_minibatches=2,\n",
    "    # Logging\n",
    "    log_interval=2,\n",
    "    eval_interval=20,\n",
    "    use_wandb=False,  # Disable for demo\n",
    ")\n",
    "\n",
    "print(\"Demo Training Configuration:\")\n",
    "print(f\"- Total timesteps: {demo_config.total_timesteps:,}\")\n",
    "print(f\"- Parallel environments: {demo_config.num_envs}\")\n",
    "print(f\"- Rollout length: {demo_config.rollout_length}\")\n",
    "print(f\"- Network type: {demo_config.network_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run mini training\n",
    "print(\"Starting mini training demo...\")\n",
    "print(\"This may take a few minutes depending on your hardware.\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(demo_config, seed=42)\n",
    "\n",
    "# Manual training loop for better control and visualization\n",
    "key, reset_key = random.split(random.PRNGKey(42))\n",
    "states = trainer.reset_fn(reset_key)\n",
    "\n",
    "training_metrics = []\n",
    "eval_metrics = []\n",
    "\n",
    "timestep = 0\n",
    "update_count = 0\n",
    "\n",
    "print(\"\\nTraining Progress:\")\n",
    "progress_bar = tqdm(total=demo_config.total_timesteps, desc=\"Training\")\n",
    "\n",
    "while timestep < demo_config.total_timesteps:\n",
    "    # Collect rollout\n",
    "    rollout_batch, states = trainer.collect_rollout(states)\n",
    "\n",
    "    # Update policy\n",
    "    key, update_key = random.split(key)\n",
    "    trainer.params, trainer.opt_state, metrics = trainer.agent.update(\n",
    "        trainer.params, trainer.opt_state, rollout_batch, update_key\n",
    "    )\n",
    "\n",
    "    # Update counters\n",
    "    timestep += demo_config.rollout_length * demo_config.num_envs\n",
    "    update_count += 1\n",
    "\n",
    "    # Store metrics\n",
    "    training_metrics.append(\n",
    "        {\n",
    "            \"timestep\": timestep,\n",
    "            \"policy_loss\": float(metrics.policy_loss),\n",
    "            \"value_loss\": float(metrics.value_loss),\n",
    "            \"entropy_loss\": float(metrics.entropy_loss),\n",
    "            \"total_loss\": float(metrics.total_loss),\n",
    "            \"mean_reward\": float(jnp.mean(rollout_batch.rewards)),\n",
    "            \"mean_value\": float(jnp.mean(rollout_batch.values)),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    if update_count % 5 == 0:\n",
    "        eval_result = trainer.evaluate(num_episodes=3)\n",
    "        eval_result[\"timestep\"] = timestep\n",
    "        eval_metrics.append(eval_result)\n",
    "\n",
    "    # Update progress\n",
    "    progress_bar.update(demo_config.rollout_length * demo_config.num_envs)\n",
    "    progress_bar.set_postfix(\n",
    "        {\n",
    "            \"Reward\": f\"{training_metrics[-1]['mean_reward']:.2f}\",\n",
    "            \"Loss\": f\"{training_metrics[-1]['total_loss']:.4f}\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "progress_bar.close()\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training results\n",
    "import pandas as pd\n",
    "\n",
    "# Convert metrics to DataFrame for easier plotting\n",
    "training_df = pd.DataFrame(training_metrics)\n",
    "eval_df = pd.DataFrame(eval_metrics)\n",
    "\n",
    "# Create comprehensive training visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Training losses\n",
    "axes[0, 0].plot(\n",
    "    training_df[\"timestep\"], training_df[\"policy_loss\"], label=\"Policy Loss\", alpha=0.7\n",
    ")\n",
    "axes[0, 0].plot(\n",
    "    training_df[\"timestep\"], training_df[\"value_loss\"], label=\"Value Loss\", alpha=0.7\n",
    ")\n",
    "axes[0, 0].set_xlabel(\"Timestep\")\n",
    "axes[0, 0].set_ylabel(\"Loss\")\n",
    "axes[0, 0].set_title(\"Training Losses\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Rewards\n",
    "axes[0, 1].plot(\n",
    "    training_df[\"timestep\"], training_df[\"mean_reward\"], alpha=0.7, color=\"green\"\n",
    ")\n",
    "axes[0, 1].set_xlabel(\"Timestep\")\n",
    "axes[0, 1].set_ylabel(\"Mean Reward\")\n",
    "axes[0, 1].set_title(\"Training Rewards\")\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Values\n",
    "axes[0, 2].plot(\n",
    "    training_df[\"timestep\"], training_df[\"mean_value\"], alpha=0.7, color=\"purple\"\n",
    ")\n",
    "axes[0, 2].set_xlabel(\"Timestep\")\n",
    "axes[0, 2].set_ylabel(\"Mean Value\")\n",
    "axes[0, 2].set_title(\"Value Function\")\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Evaluation metrics\n",
    "if len(eval_df) > 0:\n",
    "    axes[1, 0].plot(\n",
    "        eval_df[\"timestep\"],\n",
    "        eval_df[\"eval/episode_reward\"],\n",
    "        \"o-\",\n",
    "        alpha=0.7,\n",
    "        color=\"red\",\n",
    "    )\n",
    "    axes[1, 0].set_xlabel(\"Timestep\")\n",
    "    axes[1, 0].set_ylabel(\"Episode Reward\")\n",
    "    axes[1, 0].set_title(\"Evaluation - Episode Reward\")\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1, 1].plot(\n",
    "        eval_df[\"timestep\"],\n",
    "        eval_df[\"eval/episode_length\"],\n",
    "        \"o-\",\n",
    "        alpha=0.7,\n",
    "        color=\"orange\",\n",
    "    )\n",
    "    axes[1, 1].set_xlabel(\"Timestep\")\n",
    "    axes[1, 1].set_ylabel(\"Episode Length\")\n",
    "    axes[1, 1].set_title(\"Evaluation - Episode Length\")\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1, 2].plot(\n",
    "        eval_df[\"timestep\"], eval_df[\"eval/bins_used\"], \"o-\", alpha=0.7, color=\"brown\"\n",
    "    )\n",
    "    axes[1, 2].set_xlabel(\"Timestep\")\n",
    "    axes[1, 2].set_ylabel(\"Bins Used\")\n",
    "    axes[1, 2].set_title(\"Evaluation - Bins Used\")\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(\"\\nFinal Training Metrics:\")\n",
    "print(f\"- Final reward: {training_df['mean_reward'].iloc[-1]:.3f}\")\n",
    "print(f\"- Final policy loss: {training_df['policy_loss'].iloc[-1]:.4f}\")\n",
    "print(f\"- Final value loss: {training_df['value_loss'].iloc[-1]:.4f}\")\n",
    "\n",
    "if len(eval_df) > 0:\n",
    "    print(\"\\nFinal Evaluation Metrics:\")\n",
    "    print(f\"- Episode reward: {eval_df['eval/episode_reward'].iloc[-1]:.3f}\")\n",
    "    print(f\"- Episode length: {eval_df['eval/episode_length'].iloc[-1]:.1f}\")\n",
    "    print(f\"- Bins used: {eval_df['eval/bins_used'].iloc[-1]:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Trained Agent Demonstration\n",
    "\n",
    "Let's test our trained agent and compare it with the First Fit baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test trained agent vs First Fit on multiple episodes\n",
    "num_test_episodes = 10\n",
    "\n",
    "agent_results = []\n",
    "first_fit_results = []\n",
    "\n",
    "print(\"Comparing Trained Agent vs First Fit:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for episode in range(num_test_episodes):\n",
    "    # Test environment\n",
    "    test_env = BinPackingEnv(\n",
    "        bin_capacity=1.0, max_bins=10, max_items=15, item_size_range=(0.1, 0.4)\n",
    "    )\n",
    "\n",
    "    # Initialize same episode for both\n",
    "    episode_key = random.PRNGKey(episode + 100)\n",
    "    initial_state = test_env.reset(episode_key, num_items=8)\n",
    "\n",
    "    # Test trained agent\n",
    "    agent_state = initial_state\n",
    "    agent_reward = 0\n",
    "    agent_steps = 0\n",
    "\n",
    "    while not agent_state.done and agent_steps < 20:\n",
    "        valid_actions = test_env.get_valid_actions(agent_state)\n",
    "        action_key = random.PRNGKey(agent_steps + episode * 100)\n",
    "        action, _, _ = trainer.agent.select_action(\n",
    "            trainer.params, agent_state, action_key, valid_actions\n",
    "        )\n",
    "        agent_state, reward, _ = test_env.step(agent_state, action, action_key)\n",
    "        agent_reward += reward\n",
    "        agent_steps += 1\n",
    "\n",
    "    agent_bins_used = jnp.sum(agent_state.bin_utilization > 0)\n",
    "\n",
    "    # Test First Fit\n",
    "    ff_state = initial_state\n",
    "    ff_reward = 0\n",
    "    ff_steps = 0\n",
    "\n",
    "    while not ff_state.done and ff_steps < 20:\n",
    "        action_idx = first_fit_strategy(ff_state)\n",
    "        action = BinPackingAction(bin_idx=action_idx)\n",
    "        step_key = random.PRNGKey(ff_steps + episode * 100)\n",
    "        ff_state, reward, _ = test_env.step(ff_state, action, step_key)\n",
    "        ff_reward += reward\n",
    "        ff_steps += 1\n",
    "\n",
    "    ff_bins_used = jnp.sum(ff_state.bin_utilization > 0)\n",
    "\n",
    "    # Store results\n",
    "    agent_results.append(\n",
    "        {\n",
    "            \"episode\": episode,\n",
    "            \"reward\": float(agent_reward),\n",
    "            \"bins_used\": int(agent_bins_used),\n",
    "            \"steps\": agent_steps,\n",
    "            \"success\": bool(agent_state.done and agent_steps < 20),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    first_fit_results.append(\n",
    "        {\n",
    "            \"episode\": episode,\n",
    "            \"reward\": float(ff_reward),\n",
    "            \"bins_used\": int(ff_bins_used),\n",
    "            \"steps\": ff_steps,\n",
    "            \"success\": bool(ff_state.done and ff_steps < 20),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Episode {episode + 1}: Agent={agent_bins_used} bins, FF={ff_bins_used} bins\"\n",
    "    )\n",
    "\n",
    "# Convert to DataFrames\n",
    "agent_df = pd.DataFrame(agent_results)\n",
    "ff_df = pd.DataFrame(first_fit_results)\n",
    "\n",
    "print(\"\\nComparison Results:\")\n",
    "print(\n",
    "    f\"Trained Agent - Avg Bins: {agent_df['bins_used'].mean():.1f}, Avg Reward: {agent_df['reward'].mean():.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"First Fit - Avg Bins: {ff_df['bins_used'].mean():.1f}, Avg Reward: {ff_df['reward'].mean():.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Success Rate - Agent: {agent_df['success'].mean():.1%}, First Fit: {ff_df['success'].mean():.1%}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Bins used comparison\n",
    "x = np.arange(len(agent_df))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(\n",
    "    x - width / 2,\n",
    "    agent_df[\"bins_used\"],\n",
    "    width,\n",
    "    label=\"Trained Agent\",\n",
    "    alpha=0.7,\n",
    "    color=\"blue\",\n",
    ")\n",
    "ax1.bar(\n",
    "    x + width / 2, ff_df[\"bins_used\"], width, label=\"First Fit\", alpha=0.7, color=\"red\"\n",
    ")\n",
    "ax1.set_xlabel(\"Episode\")\n",
    "ax1.set_ylabel(\"Bins Used\")\n",
    "ax1.set_title(\"Bins Used Comparison\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Reward comparison\n",
    "ax2.bar(\n",
    "    x - width / 2,\n",
    "    agent_df[\"reward\"],\n",
    "    width,\n",
    "    label=\"Trained Agent\",\n",
    "    alpha=0.7,\n",
    "    color=\"blue\",\n",
    ")\n",
    "ax2.bar(\n",
    "    x + width / 2, ff_df[\"reward\"], width, label=\"First Fit\", alpha=0.7, color=\"red\"\n",
    ")\n",
    "ax2.set_xlabel(\"Episode\")\n",
    "ax2.set_ylabel(\"Total Reward\")\n",
    "ax2.set_title(\"Reward Comparison\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot comparison\n",
    "bins_data = [agent_df[\"bins_used\"], ff_df[\"bins_used\"]]\n",
    "ax3.boxplot(bins_data, labels=[\"Trained Agent\", \"First Fit\"])\n",
    "ax3.set_ylabel(\"Bins Used\")\n",
    "ax3.set_title(\"Bins Used Distribution\")\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical comparison\n",
    "from scipy import stats\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_value = stats.ttest_ind(agent_df[\"bins_used\"], ff_df[\"bins_used\"])\n",
    "print(\"\\nStatistical Test (t-test):\")\n",
    "print(f\"T-statistic: {t_stat:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "print(f\"Significant difference: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "\n",
    "# Improvement calculation\n",
    "improvement = (\n",
    "    (ff_df[\"bins_used\"].mean() - agent_df[\"bins_used\"].mean())\n",
    "    / ff_df[\"bins_used\"].mean()\n",
    "    * 100\n",
    ")\n",
    "print(f\"\\nAgent improvement over First Fit: {improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Analysis\n",
    "\n",
    "Let's analyze the computational performance of our JAX implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance benchmarking\n",
    "print(\"Performance Benchmarking:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Environment step performance\n",
    "test_env = BinPackingEnv()\n",
    "test_state = test_env.reset(random.PRNGKey(0))\n",
    "test_action = BinPackingAction(bin_idx=0)\n",
    "\n",
    "# Warmup\n",
    "for _ in range(10):\n",
    "    test_env.step(test_state, test_action, random.PRNGKey(0))\n",
    "\n",
    "# Benchmark environment steps\n",
    "num_steps = 1000\n",
    "start_time = time.time()\n",
    "for i in range(num_steps):\n",
    "    test_env.step(test_state, test_action, random.PRNGKey(i))\n",
    "env_time = time.time() - start_time\n",
    "\n",
    "print(f\"Environment steps: {num_steps / env_time:.0f} steps/sec\")\n",
    "\n",
    "# Benchmark network forward pass\n",
    "network = create_network(\"simple\", hidden_dim=64, max_bins=10)\n",
    "params = network.init(random.PRNGKey(0), test_state, training=False)\n",
    "\n",
    "# Warmup\n",
    "for _ in range(10):\n",
    "    network.apply(params, test_state, training=False)\n",
    "\n",
    "# Benchmark network\n",
    "start_time = time.time()\n",
    "for _ in range(num_steps):\n",
    "    network.apply(params, test_state, training=False)\n",
    "net_time = time.time() - start_time\n",
    "\n",
    "print(f\"Network forward pass: {num_steps / net_time:.0f} forward/sec\")\n",
    "\n",
    "# Benchmark vectorized operations\n",
    "from binax.environment import make_vectorized_env\n",
    "\n",
    "env_params = {\n",
    "    \"bin_capacity\": 1.0,\n",
    "    \"max_bins\": 10,\n",
    "    \"max_items\": 20,\n",
    "    \"item_size_range\": (0.1, 0.5),\n",
    "}\n",
    "\n",
    "num_envs = 32\n",
    "reset_fn, step_fn, _ = make_vectorized_env(env_params, num_envs)\n",
    "\n",
    "# Benchmark vectorized steps\n",
    "vec_states = reset_fn(random.PRNGKey(0))\n",
    "vec_actions = BinPackingAction(bin_idx=jnp.zeros(num_envs, dtype=jnp.int32))\n",
    "\n",
    "# Warmup\n",
    "for _ in range(10):\n",
    "    step_fn(vec_states, vec_actions, random.PRNGKey(0))\n",
    "\n",
    "num_vec_steps = 100\n",
    "start_time = time.time()\n",
    "for i in range(num_vec_steps):\n",
    "    step_fn(vec_states, vec_actions, random.PRNGKey(i))\n",
    "vec_time = time.time() - start_time\n",
    "\n",
    "effective_steps = num_vec_steps * num_envs\n",
    "print(f\"Vectorized steps: {effective_steps / vec_time:.0f} steps/sec ({num_envs} envs)\")\n",
    "\n",
    "# Memory usage estimation\n",
    "param_size = sum(x.size for x in jax.tree_leaves(params)) * 4  # 4 bytes per float32\n",
    "state_size = sum(x.size for x in jax.tree_leaves(test_state)) * 4\n",
    "\n",
    "print(\"\\nMemory Usage:\")\n",
    "print(f\"- Network parameters: {param_size / 1024:.1f} KB\")\n",
    "print(f\"- Single state: {state_size} bytes\")\n",
    "print(f\"- {num_envs} states: {state_size * num_envs / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "This demo showcased the key features of BinAX:\n",
    "\n",
    "### Key Highlights:\n",
    "\n",
    "1. **Environment**: Flexible bin packing environment with configurable parameters\n",
    "2. **Networks**: Both attention-based and simple architectures\n",
    "3. **Algorithm**: Complete PPO implementation with GAE\n",
    "4. **Performance**: JAX-powered high-performance computing\n",
    "5. **Evaluation**: Comprehensive metrics and comparisons\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Scale up training with more environments and longer training\n",
    "- Experiment with different network architectures\n",
    "- Test on more complex bin packing variants\n",
    "- Compare with other RL algorithms\n",
    "- Benchmark against more sophisticated heuristics\n",
    "\n",
    "### Usage:\n",
    "\n",
    "```python\n",
    "# For full-scale training\n",
    "config = TrainingConfig(\n",
    "    total_timesteps=1_000_000,\n",
    "    num_envs=64,\n",
    "    network_type=\"attention\",\n",
    "    use_wandb=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(config)\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "BinAX provides a solid foundation for reinforcement learning research in combinatorial optimization!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
