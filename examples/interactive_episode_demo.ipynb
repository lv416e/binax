{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Episode Visualization Demo\n",
    "\n",
    "This notebook demonstrates the enhanced visualization capabilities for BinAX episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from binax.environment import BinPackingEnv\n",
    "from binax.algorithms import PPOAgent, PPOConfig\n",
    "from binax.networks import SimplePolicyValueNetwork\n",
    "from binax.types import BinPackingAction\n",
    "from binax.visualizer import EpisodeVisualizer\n",
    "from binax.interactive_viz import InteractiveEpisodeExplorer, EpisodeComparator\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "env = BinPackingEnv(\n",
    "    bin_capacity=1.0,\n",
    "    max_bins=50,\n",
    "    max_items=15,\n",
    "    item_size_range=(0.1, 0.6)\n",
    ")\n",
    "\n",
    "# Initialize agent\n",
    "network = SimplePolicyValueNetwork(\n",
    "    hidden_dims=[64, 64],\n",
    "    max_bins=env.max_bins\n",
    ")\n",
    "\n",
    "config = PPOConfig(\n",
    "    learning_rate=3e-4,\n",
    "    clip_eps=0.2,\n",
    "    value_loss_coeff=0.5,\n",
    "    entropy_coeff=0.01\n",
    ")\n",
    "\n",
    "agent = PPOAgent(network=network, config=config)\n",
    "\n",
    "# Initialize network parameters\n",
    "key = jax.random.PRNGKey(42)\n",
    "dummy_state = env.reset(key)\n",
    "network_params = agent.init_params(key, dummy_state)\n",
    "\n",
    "print(\"Environment and agent initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Episode with Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_visualized_episode(env, agent, network_params, visualizer, seed=0):\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    key, reset_key = jax.random.split(key)\n",
    "    \n",
    "    state = env.reset(reset_key)\n",
    "    visualizer.clear_history()\n",
    "    \n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    \n",
    "    while not state.done:\n",
    "        key, action_key, step_key = jax.random.split(key, 3)\n",
    "        \n",
    "        # Get valid actions and network output\n",
    "        valid_actions = env.get_valid_actions(state)\n",
    "        network_output = agent.network.apply(network_params, state, training=False)\n",
    "        \n",
    "        # Mask invalid actions and get probabilities\n",
    "        masked_logits = jnp.where(valid_actions, network_output.action_logits, -1e9)\n",
    "        action_probs = jax.nn.softmax(masked_logits)\n",
    "        \n",
    "        # Sample action\n",
    "        action_idx = jax.random.categorical(action_key, masked_logits)\n",
    "        action = BinPackingAction(bin_idx=action_idx)\n",
    "        \n",
    "        # Step environment\n",
    "        next_state, reward, _ = env.step(state, action, step_key)\n",
    "        \n",
    "        # Record step for visualization\n",
    "        visualizer.record_step(\n",
    "            state=state,\n",
    "            action=action,\n",
    "            reward=float(reward),\n",
    "            action_probs=np.array(action_probs),\n",
    "            value_estimate=float(network_output.value)\n",
    "        )\n",
    "        \n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "        state = next_state\n",
    "        \n",
    "        current_item = float(state.item_queue[state.current_item_idx-1])\n",
    "        prob = float(action_probs[action_idx])\n",
    "        print(f\"Step {step_count}: Item {current_item:.3f} â†’ Bin {action.bin_idx} (prob: {prob:.2f}), Reward: {reward:.3f}\")\n",
    "    \n",
    "    print(f\"\\nEpisode completed in {step_count} steps with total reward: {total_reward:.3f}\")\n",
    "    return total_reward\n",
    "\n",
    "# Create visualizer and run episode\n",
    "visualizer = EpisodeVisualizer(bin_capacity=env.bin_capacity, max_bins=env.max_bins)\n",
    "total_reward = run_visualized_episode(env, agent, network_params, visualizer, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode Summary Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualizer.plot_episode_summary(figsize=(16, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Explorer (if ipywidgets available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    explorer = InteractiveEpisodeExplorer(visualizer)\n",
    "    explorer.display()\n",
    "except ImportError:\n",
    "    print(\"ipywidgets not available. Install with: uv add ipywidgets\")\n",
    "    print(\"Showing static key steps instead.\")\n",
    "    \n",
    "    num_steps = len(visualizer.episode_history)\n",
    "    key_steps = [0, num_steps//3, 2*num_steps//3, num_steps-1]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, step_idx in enumerate(key_steps):\n",
    "        ax = axes[i]\n",
    "        step = visualizer.episode_history[step_idx]\n",
    "        \n",
    "        if step.action_probs is not None:\n",
    "            x = np.arange(min(10, len(step.action_probs)))\n",
    "            probs = step.action_probs[:len(x)]\n",
    "            colors = ['red' if j == step.bin_selected and j < len(x) else 'skyblue' for j in range(len(x))]\n",
    "            ax.bar(x, probs, color=colors)\n",
    "            ax.set_title(f'Step {step_idx+1}: Item {step.item_size:.3f}')\n",
    "            ax.set_xlabel('Bin Index')\n",
    "            ax.set_ylabel('Probability')\n",
    "            ax.set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = visualizer.create_episode_animation(interval=800, figsize=(12, 8))\n",
    "if anim:\n",
    "    print(\"Animation created successfully!\")\n",
    "    # Note: Animation display may require additional setup in some environments\n",
    "else:\n",
    "    print(\"Animation creation failed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
