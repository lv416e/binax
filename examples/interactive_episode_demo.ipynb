{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Episode Visualization Demo\n",
    "\n",
    "This notebook demonstrates the enhanced visualization capabilities for BinAX episodes, showing how the agent makes decisions step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from binax.environment import BinPackingEnv\n",
    "from binax.algorithms import PPO\n",
    "from binax.networks import SimpleNetwork\n",
    "from binax.visualizer import EpisodeVisualizer\n",
    "from binax.interactive_viz import InteractiveEpisodeExplorer, EpisodeComparator\n",
    "\n",
    "# Enable interactive plots\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "env_params = {\n",
    "    \"num_items\": 15,\n",
    "    \"item_dist\": \"uniform\",\n",
    "    \"item_sizes\": (0.1, 0.6),\n",
    "    \"bin_capacity\": 1.0,\n",
    "    \"reward_type\": \"utilization\",\n",
    "}\n",
    "env = BinPackingEnv(**env_params)\n",
    "\n",
    "# Initialize agent\n",
    "network = SimpleNetwork(\n",
    "    hidden_dim=64,\n",
    "    num_layers=2,\n",
    "    max_bins=env.max_bins,\n",
    ")\n",
    "\n",
    "agent = PPO(\n",
    "    network=network,\n",
    "    learning_rate=3e-4,\n",
    "    clip_epsilon=0.2,\n",
    "    value_coef=0.5,\n",
    "    entropy_coef=0.01,\n",
    ")\n",
    "\n",
    "# Initialize network parameters\n",
    "key = jax.random.PRNGKey(42)\n",
    "dummy_state = env.reset(key)\n",
    "network_params = network.init(key, dummy_state)\n",
    "agent.network_params = network_params\n",
    "\n",
    "print(\"Environment and agent initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Episode with Visualization Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_visualized_episode(env, agent, visualizer, seed=0, verbose=True):\n",
    "    \"\"\"Run a single episode while recording visualization data.\"\"\"\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    key, reset_key = jax.random.split(key)\n",
    "    \n",
    "    # Reset environment\n",
    "    state = env.reset(reset_key)\n",
    "    visualizer.clear_history()\n",
    "    \n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Starting episode...\\n\")\n",
    "    \n",
    "    while not state.done:\n",
    "        key, action_key = jax.random.split(key)\n",
    "        \n",
    "        # Get action from agent with probabilities\n",
    "        action_logits = agent.network.apply(\n",
    "            agent.network_params, state, method=agent.network.policy\n",
    "        )\n",
    "        action_probs = jax.nn.softmax(action_logits)\n",
    "        \n",
    "        # Sample action\n",
    "        action_idx = jax.random.categorical(action_key, action_logits)\n",
    "        action = env.idx_to_action(action_idx, state)\n",
    "        \n",
    "        # Get value estimate\n",
    "        value = agent.network.apply(\n",
    "            agent.network_params, state, method=agent.network.value\n",
    "        )\n",
    "        \n",
    "        # Step environment\n",
    "        next_state, reward = env.step(state, action)\n",
    "        \n",
    "        # Record step for visualization\n",
    "        visualizer.record_step(\n",
    "            state=state,\n",
    "            action=action,\n",
    "            reward=float(reward),\n",
    "            action_probs=np.array(action_probs),\n",
    "            value_estimate=float(value)\n",
    "        )\n",
    "        \n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "        \n",
    "        if verbose:\n",
    "            item_size = float(state.item_queue[state.current_item_idx])\n",
    "            prob = float(action_probs[action_idx])\n",
    "            print(f\"Step {step_count}: Item {item_size:.3f} → Bin {action.bin_index} \"\n",
    "                  f\"(prob: {prob:.2f}), Reward: {reward:.3f}\")\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nEpisode completed in {step_count} steps with total reward: {total_reward:.3f}\")\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "# Create visualizer and run episode\n",
    "visualizer = EpisodeVisualizer(\n",
    "    bin_capacity=env.bin_capacity,\n",
    "    max_bins=env.max_bins\n",
    ")\n",
    "\n",
    "total_reward = run_visualized_episode(env, agent, visualizer, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode Summary Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive episode summary\n",
    "fig = visualizer.plot_episode_summary(figsize=(16, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Episode Explorer\n",
    "\n",
    "Use the controls below to step through the episode and see how the agent made each decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive explorer\n",
    "explorer = InteractiveEpisodeExplorer(visualizer)\n",
    "explorer.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animated Episode Playback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create animation\n",
    "anim = visualizer.create_episode_animation(interval=800, figsize=(12, 8))\n",
    "if anim:\n",
    "    from IPython.display import HTML\n",
    "    HTML(anim.to_jshtml())\n",
    "else:\n",
    "    print(\"No animation data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Multiple Episodes\n",
    "\n",
    "Let's run several episodes and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple episodes for comparison\n",
    "visualizers = []\n",
    "rewards = []\n",
    "\n",
    "for i in range(3):\n",
    "    viz = EpisodeVisualizer(\n",
    "        bin_capacity=env.bin_capacity,\n",
    "        max_bins=env.max_bins\n",
    "    )\n",
    "    \n",
    "    reward = run_visualized_episode(env, agent, viz, seed=i*123, verbose=False)\n",
    "    \n",
    "    visualizers.append(viz)\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    print(f\"Episode {i+1}: {len(viz.episode_history)} steps, reward: {reward:.3f}\")\n",
    "\n",
    "print(f\"\\nAverage reward: {np.mean(rewards):.3f} ± {np.std(rewards):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare episodes\n",
    "comparator = EpisodeComparator(\n",
    "    visualizers=visualizers,\n",
    "    labels=[f\"Episode {i+1}\" for i in range(len(visualizers))]\n",
    ")\n",
    "\n",
    "# Show comparison plot\n",
    "fig = comparator.compare_episodes(figsize=(18, 12))\n",
    "plt.show()\n",
    "\n",
    "# Show performance summary\n",
    "print(\"\\nPerformance Summary:\")\n",
    "summary = comparator.create_performance_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Agent Decision Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze decision patterns from the first episode\n",
    "first_viz = visualizers[0]\n",
    "\n",
    "# Plot distribution of action probabilities\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Action probability distribution\n",
    "ax = axes[0, 0]\n",
    "all_probs = []\n",
    "for step in first_viz.episode_history:\n",
    "    if step.action_probs is not None:\n",
    "        # Get probability of selected action\n",
    "        selected_prob = step.action_probs[step.bin_selected]\n",
    "        all_probs.append(selected_prob)\n",
    "\n",
    "ax.hist(all_probs, bins=20, alpha=0.7, color='skyblue')\n",
    "ax.set_xlabel('Selected Action Probability')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution of Selected Action Probabilities')\n",
    "ax.axvline(np.mean(all_probs), color='red', linestyle='--', label=f'Mean: {np.mean(all_probs):.2f}')\n",
    "ax.legend()\n",
    "\n",
    "# 2. Value estimates over time\n",
    "ax = axes[0, 1]\n",
    "values = [step.value_estimate for step in first_viz.episode_history if step.value_estimate is not None]\n",
    "ax.plot(values, color='green', linewidth=2)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Value Estimate')\n",
    "ax.set_title('Agent Value Estimates')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Item size vs action confidence\n",
    "ax = axes[1, 0]\n",
    "item_sizes = [step.item_size for step in first_viz.episode_history]\n",
    "action_confidence = [max(step.action_probs) if step.action_probs is not None else 0 \n",
    "                    for step in first_viz.episode_history]\n",
    "\n",
    "ax.scatter(item_sizes, action_confidence, alpha=0.7, color='purple')\n",
    "ax.set_xlabel('Item Size')\n",
    "ax.set_ylabel('Max Action Probability')\n",
    "ax.set_title('Item Size vs Action Confidence')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Reward distribution\n",
    "ax = axes[1, 1]\n",
    "rewards = [step.reward for step in first_viz.episode_history]\n",
    "ax.hist(rewards, bins=15, alpha=0.7, color='orange')\n",
    "ax.set_xlabel('Reward')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Reward Distribution')\n",
    "ax.axvline(np.mean(rewards), color='red', linestyle='--', label=f'Mean: {np.mean(rewards):.3f}')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDecision Analysis:\")\n",
    "print(f\"Average action confidence: {np.mean(action_confidence):.3f}\")\n",
    "print(f\"Most confident decision: {max(action_confidence):.3f}\")\n",
    "print(f\"Least confident decision: {min(action_confidence):.3f}\")\n",
    "print(f\"Average value estimate: {np.mean(values):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save static visualizations\n",
    "fig_summary = visualizers[0].plot_episode_summary(figsize=(16, 10))\n",
    "fig_summary.savefig('episode_summary.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Saved episode_summary.png\")\n",
    "\n",
    "# Save comparison\n",
    "fig_comparison = comparator.compare_episodes(figsize=(18, 12))\n",
    "fig_comparison.savefig('episode_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Saved episode_comparison.png\")\n",
    "\n",
    "# Try to save animation\n",
    "anim = visualizers[0].create_episode_animation(interval=1000, figsize=(10, 8))\n",
    "if anim:\n",
    "    try:\n",
    "        anim.save('episode_animation.gif', writer='pillow', fps=1)\n",
    "        print(\"Saved episode_animation.gif\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not save animation: {e}\")\n",
    "        print(\"Install pillow for GIF support: pip install pillow\")\n",
    "        \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
