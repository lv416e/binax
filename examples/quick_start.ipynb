{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BinAX Quick Start Guide\n",
    "\n",
    "This notebook provides a quick introduction to BinAX for getting started with bin packing reinforcement learning.\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "# Import BinAX\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from binax import BinPackingEnv, PolicyValueNetwork, PPOAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bin packing environment\n",
    "env = BinPackingEnv(\n",
    "    bin_capacity=1.0, max_bins=10, max_items=20, item_size_range=(0.1, 0.5)\n",
    ")\n",
    "\n",
    "# Reset environment\n",
    "key = random.PRNGKey(42)\n",
    "state = env.reset(key, num_items=8)\n",
    "\n",
    "print(env.render_state(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Network and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network\n",
    "network = PolicyValueNetwork(hidden_dim=128, num_layers=2, max_bins=10)\n",
    "\n",
    "# Create PPO agent\n",
    "agent = PPOAgent(network, action_dim=11)\n",
    "\n",
    "# Initialize parameters\n",
    "key, param_key = random.split(key)\n",
    "params = agent.init_params(param_key, state)\n",
    "\n",
    "print(f\"Network parameters: {sum(x.size for x in jax.tree_leaves(params)):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test agent action selection\n",
    "valid_actions = env.get_valid_actions(state)\n",
    "key, action_key = random.split(key)\n",
    "\n",
    "action, log_prob, value = agent.select_action(params, state, action_key, valid_actions)\n",
    "\n",
    "print(f\"Selected action: {action.bin_idx}\")\n",
    "print(f\"Log probability: {log_prob:.4f}\")\n",
    "print(f\"Value estimate: {value:.4f}\")\n",
    "print(f\"Valid actions: {valid_actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a complete episode\n",
    "current_state = env.reset(random.PRNGKey(123), num_items=6)\n",
    "episode_reward = 0\n",
    "steps = 0\n",
    "\n",
    "print(\"Running episode...\")\n",
    "while not current_state.done and steps < 20:\n",
    "    valid_actions = env.get_valid_actions(current_state)\n",
    "    key, action_key = random.split(key)\n",
    "\n",
    "    action, _, _ = agent.select_action(params, current_state, action_key, valid_actions)\n",
    "\n",
    "    next_state, reward, done = env.step(current_state, action, action_key)\n",
    "\n",
    "    print(\n",
    "        f\"Step {steps}: Item {current_state.item_queue[current_state.current_item_idx]:.3f} -> Bin {action.bin_idx}, Reward: {reward:.2f}\"\n",
    "    )\n",
    "\n",
    "    episode_reward += reward\n",
    "    current_state = next_state\n",
    "    steps += 1\n",
    "\n",
    "print(\"\\nEpisode completed!\")\n",
    "print(f\"Total reward: {episode_reward:.2f}\")\n",
    "print(f\"Bins used: {jnp.sum(current_state.bin_utilization > 0)}\")\n",
    "print(\"\\nFinal state:\")\n",
    "print(env.render_state(current_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "\n",
    "For full training, use the trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from binax.trainer import Trainer, TrainingConfig\n",
    "\n",
    "# Create training configuration\n",
    "config = TrainingConfig(\n",
    "    total_timesteps=50_000,  # Small for demo\n",
    "    num_envs=16,\n",
    "    network_type=\"simple\",\n",
    "    use_wandb=False,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(config, seed=42)\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "# trainer.train()  # Uncomment to run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Full Training**: Run `python -m binax.trainer` for complete training\n",
    "2. **Experiments**: Try different network architectures and hyperparameters\n",
    "3. **Evaluation**: Compare with classical heuristics\n",
    "4. **Scaling**: Use more environments and longer training\n",
    "\n",
    "See the full demo notebook for more advanced examples!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
